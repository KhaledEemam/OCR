{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3919937,"sourceType":"datasetVersion","datasetId":2327240},{"sourceId":8844234,"sourceType":"datasetVersion","datasetId":5323136}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing Packages and libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport numpy as np\nimport os\nimport ast\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import ImageOps , Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:16:50.031481Z","iopub.execute_input":"2024-07-02T21:16:50.031837Z","iopub.status.idle":"2024-07-02T21:16:57.399111Z","shell.execute_reply.started":"2024-07-02T21:16:50.031809Z","shell.execute_reply":"2024-07-02T21:16:57.398331Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Data**","metadata":{}},{"cell_type":"code","source":"AANOTATIONS_PATH = '/kaggle/input/textocr-text-extraction-from-images-dataset/annot.csv'\nIMAGES_INFO_PATH = '/kaggle/input/textocr-text-extraction-from-images-dataset/img.csv'\nIMAGES_PATH = '/kaggle/input/textocr-text-extraction-from-images-dataset/train_val_images/train_images'","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:17:39.098899Z","iopub.execute_input":"2024-07-02T21:17:39.099607Z","iopub.status.idle":"2024-07-02T21:17:39.103801Z","shell.execute_reply.started":"2024-07-02T21:17:39.099573Z","shell.execute_reply":"2024-07-02T21:17:39.102887Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"images_annotations = pd.read_csv(AANOTATIONS_PATH, encoding='utf-8')\nimages_info_data = pd.read_csv(IMAGES_INFO_PATH, encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:17:39.629229Z","iopub.execute_input":"2024-07-02T21:17:39.629855Z","iopub.status.idle":"2024-07-02T21:17:44.100301Z","shell.execute_reply.started":"2024-07-02T21:17:39.629825Z","shell.execute_reply":"2024-07-02T21:17:44.099513Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"max_width , max_height = 500 , 500\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:17:44.101822Z","iopub.execute_input":"2024-07-02T21:17:44.102126Z","iopub.status.idle":"2024-07-02T21:17:44.127820Z","shell.execute_reply.started":"2024-07-02T21:17:44.102099Z","shell.execute_reply":"2024-07-02T21:17:44.126923Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing Data**","metadata":{}},{"cell_type":"code","source":"def preprocess_text_recognizer_data(images_annotations, max_width, max_height):\n    data = images_annotations[['image_id', 'bbox', 'utf8_string']]\n    images, targets = [], []\n    #chars = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%^&*()-_=+[]{}|;:'\\\",.<>?/\\\\`~\\n\")\n    chars = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n    letter_to_index = {letter: index for index, letter in enumerate(chars)}\n    index_to_letters = {index: letter for letter, index in letter_to_index.items()}\n    wrong_annotated_images = ['0c6eb2e5cff08c2b','1f7ad7273543715d','260b8a281ff565ed','5bcbdb5d7d9f5f4a','2ceb34939bf4133c']\n    wrong_annotated_set = set()\n    for image in wrong_annotated_images :\n        wrong_annotated_set.add(image)\n    \n    for _, row in tqdm(data.iterrows()):\n        if row['image_id'] not in  wrong_annotated_set :\n            encoded_text = [letter_to_index[letter] for letter in str(row['utf8_string']) if letter in chars]\n            if len(encoded_text) == len(str(row['utf8_string'])) and len(encoded_text) <= 15 :\n                images.append(row['image_id'])\n                targets.append({\"box\": ast.literal_eval(row['bbox']), \"text\": encoded_text})\n            \n    \n    return images, targets, index_to_letters","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:17:49.699944Z","iopub.execute_input":"2024-07-02T21:17:49.700484Z","iopub.status.idle":"2024-07-02T21:17:49.711626Z","shell.execute_reply.started":"2024-07-02T21:17:49.700447Z","shell.execute_reply":"2024-07-02T21:17:49.710688Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"images, boxes_and_labels, index_to_letters = preprocess_text_recognizer_data(images_annotations, max_width, max_height)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:17:51.494607Z","iopub.execute_input":"2024-07-02T21:17:51.495305Z","iopub.status.idle":"2024-07-02T21:19:18.712804Z","shell.execute_reply.started":"2024-07-02T21:17:51.495270Z","shell.execute_reply":"2024-07-02T21:19:18.711884Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"1052354it [01:27, 12076.19it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Splitting Data**","metadata":{}},{"cell_type":"code","source":"training_images, testing_images, training_targets, testing_targets = train_test_split(images , boxes_and_labels , test_size= .2 , shuffle= True)\nvalidation_images, testing_images, validation_targets, testing_targets = train_test_split(testing_images , testing_targets  , test_size= .5 , shuffle= True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:18.714784Z","iopub.execute_input":"2024-07-02T21:19:18.715443Z","iopub.status.idle":"2024-07-02T21:19:19.105984Z","shell.execute_reply.started":"2024-07-02T21:19:18.715408Z","shell.execute_reply":"2024-07-02T21:19:19.105234Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Data Loading**","metadata":{}},{"cell_type":"code","source":"batch_size = 1\nnum_workers = 1\npin_memory = True\nshuffle = True\nepochs = 2","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.107020Z","iopub.execute_input":"2024-07-02T21:19:19.107305Z","iopub.status.idle":"2024-07-02T21:19:19.111578Z","shell.execute_reply.started":"2024-07-02T21:19:19.107281Z","shell.execute_reply":"2024-07-02T21:19:19.110731Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def resize_image_and_padding(image,max_height,max_width) :\n    resized_image = cv2.resize(image,(max_width,max_height))\n    return resized_image","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.114218Z","iopub.execute_input":"2024-07-02T21:19:19.114552Z","iopub.status.idle":"2024-07-02T21:19:19.122110Z","shell.execute_reply.started":"2024-07-02T21:19:19.114520Z","shell.execute_reply":"2024-07-02T21:19:19.121234Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_path,max_height,max_width,bbox=None) :\n\n    image = cv2.imread(image_path)\n\n    if bbox != None :\n        x , y , w , h = bbox\n        x , y , w , h = int(x) , int(y) , int(w) , int(h)\n        image = image[y:y+h,x:x+w]\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = resize_image_and_padding(image,max_height,max_width)\n    image = image / 255\n    image = torch.from_numpy(image)\n    image = image.permute(2, 0, 1)\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.123168Z","iopub.execute_input":"2024-07-02T21:19:19.123511Z","iopub.status.idle":"2024-07-02T21:19:19.167081Z","shell.execute_reply.started":"2024-07-02T21:19:19.123464Z","shell.execute_reply":"2024-07-02T21:19:19.166355Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class CustomRecognizerDataset(Dataset) :\n    def __init__(self,images,targets,max_height,max_width) :\n        self.max_height = max_height\n        self.max_width = max_width\n        self.image_id , self.targets = images , targets\n\n    def __len__(self) :\n        return len(self.image_id)\n    \n    def __getitem__(self,index) :\n        image_path = os.path.join(IMAGES_PATH,self.image_id[index]+'.jpg')  \n        image = preprocess_image(image_path,self.max_height,self.max_width,bbox=self.targets[index]['box'])\n        encoding = torch.tensor(self.targets[index]['text'])\n        \n        return {\"images\" : image , \"targets\" : encoding}","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.168022Z","iopub.execute_input":"2024-07-02T21:19:19.168315Z","iopub.status.idle":"2024-07-02T21:19:19.181946Z","shell.execute_reply.started":"2024-07-02T21:19:19.168291Z","shell.execute_reply":"2024-07-02T21:19:19.181105Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class getRecognizerLoader :\n    def __init__(self,images,targets , num_workers , shuffle , pin_memory ,max_height,max_width ,batch_size = 1 ) :\n        self.dataset = CustomRecognizerDataset(images,targets,max_height,max_width)\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.pin_memory = pin_memory\n        self.shuffle = shuffle\n\n    def get_loader(self) :\n        return DataLoader(self.dataset , batch_size = self.batch_size , shuffle = self.shuffle , num_workers = self.num_workers , pin_memory = self.pin_memory)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.183144Z","iopub.execute_input":"2024-07-02T21:19:19.183478Z","iopub.status.idle":"2024-07-02T21:19:19.192978Z","shell.execute_reply.started":"2024-07-02T21:19:19.183449Z","shell.execute_reply":"2024-07-02T21:19:19.192139Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"training_data_loader = getRecognizerLoader(images=training_images, targets=training_targets , num_workers = num_workers \n                                 , shuffle=shuffle , pin_memory = pin_memory ,max_height= max_height ,max_width= max_width ).get_loader()\nvalidation_data_loader = getRecognizerLoader(images=validation_images, targets=validation_targets  , num_workers = num_workers \n                                   , shuffle=shuffle , pin_memory = pin_memory ,max_height= max_height ,max_width= max_width).get_loader()\ntesting_data_loader = getRecognizerLoader(images=testing_images, targets=testing_targets  , num_workers = num_workers \n                                , shuffle=True , pin_memory = pin_memory ,max_height= max_height ,max_width= max_width).get_loader()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.194074Z","iopub.execute_input":"2024-07-02T21:19:19.194331Z","iopub.status.idle":"2024-07-02T21:19:19.202580Z","shell.execute_reply.started":"2024-07-02T21:19:19.194309Z","shell.execute_reply":"2024-07-02T21:19:19.201771Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Building Model**","metadata":{}},{"cell_type":"code","source":"hidden_size, lstm_layers, num_classes = 256 , 2 , len(index_to_letters) + 1","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.203613Z","iopub.execute_input":"2024-07-02T21:19:19.203883Z","iopub.status.idle":"2024-07-02T21:19:19.215156Z","shell.execute_reply.started":"2024-07-02T21:19:19.203860Z","shell.execute_reply":"2024-07-02T21:19:19.214243Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"num_classes","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.218061Z","iopub.execute_input":"2024-07-02T21:19:19.218406Z","iopub.status.idle":"2024-07-02T21:19:19.227228Z","shell.execute_reply.started":"2024-07-02T21:19:19.218379Z","shell.execute_reply":"2024-07-02T21:19:19.226450Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"63"},"metadata":{}}]},{"cell_type":"code","source":"class TextRecognizer(nn.Module) :\n    def __init__(self, hidden_size, lstm_layers, num_classes):\n        super(TextRecognizer,self).__init__()\n        vgg16 = models.vgg16(pretrained = True)\n        self.cnn = nn.Sequential(*list(vgg16.children())[:-2])\n        self.hidden_size = hidden_size\n        self.num_layers = lstm_layers\n        self.lstm = nn.LSTM(7680 , hidden_size // 2, lstm_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size , num_classes)\n\n\n    def forward(self,image) :\n        output = self.cnn(image)\n        batch_size , channels , height, width= output.size()\n\n        output = output.permute(0, 3, 1, 2)  # Permute dimensions to (batch_size, height, width, channels)\n        output = output.reshape(batch_size, width ,  height * channels )\n        output , _ = self.lstm(output)\n        output = self.fc(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.228261Z","iopub.execute_input":"2024-07-02T21:19:19.228578Z","iopub.status.idle":"2024-07-02T21:19:19.237004Z","shell.execute_reply.started":"2024-07-02T21:19:19.228546Z","shell.execute_reply":"2024-07-02T21:19:19.236228Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"text_recognizer_model = TextRecognizer(hidden_size, lstm_layers, num_classes)\ntext_recognizer_model = text_recognizer_model.to(device)\n\n# Ensure all parameters are trainable\nfor param in text_recognizer_model.parameters() :\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:19.238047Z","iopub.execute_input":"2024-07-02T21:19:19.238335Z","iopub.status.idle":"2024-07-02T21:19:24.775029Z","shell.execute_reply.started":"2024-07-02T21:19:19.238312Z","shell.execute_reply":"2024-07-02T21:19:24.773961Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 165MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"SAVED_TEXT_RECOGNIZER_MODEL_PATH = '/kaggle/input/text-tecognizer-model/text_recognizer_best_model.bin'\ntext_recognizer_model.load_state_dict(torch.load(SAVED_TEXT_RECOGNIZER_MODEL_PATH))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:24.776470Z","iopub.execute_input":"2024-07-02T21:19:24.776824Z","iopub.status.idle":"2024-07-02T21:19:25.641220Z","shell.execute_reply.started":"2024-07-02T21:19:24.776793Z","shell.execute_reply":"2024-07-02T21:19:25.640142Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(text_recognizer_model.parameters(),lr = 0.0001)\nbest_score = float('inf')\nprint(len(training_data_loader))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:25.642157Z","iopub.execute_input":"2024-07-02T21:19:25.642434Z","iopub.status.idle":"2024-07-02T21:19:25.648457Z","shell.execute_reply.started":"2024-07-02T21:19:25.642412Z","shell.execute_reply":"2024-07-02T21:19:25.647556Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"471392\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0001","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:25.649565Z","iopub.execute_input":"2024-07-02T21:19:25.649873Z","iopub.status.idle":"2024-07-02T21:19:25.658932Z","shell.execute_reply.started":"2024-07-02T21:19:25.649843Z","shell.execute_reply":"2024-07-02T21:19:25.658120Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **Training and Evaluation functions**","metadata":{}},{"cell_type":"code","source":"def train_text_recognizer(model,data_loader,optimizer,device,blank_index) :\n    losses = []\n    model.train()\n\n    for i in tqdm(data_loader) :\n        optimizer.zero_grad()\n        criteration = nn.CTCLoss(blank=blank_index,zero_infinity =True)\n        image = i[\"images\"].to(device).to(torch.float32)\n        targets = i[\"targets\"].to(device)\n\n        output = model(image)\n        output = output.permute(1,0,2)\n        output = output.log_softmax(2)\n        input_lengths = torch.tensor([output.size(0)] * output.size(1))\n        target_lengths = torch.tensor([targets.size(1)] * targets.size(0))\n\n        loss = criteration(output,targets,input_lengths,target_lengths)\n        losses.append(loss.item())\n        loss.backward()\n        #Gradient Clipping\n        torch.nn.utils.clip_grad_norm_(text_recognizer_model.parameters(), max_norm=5.0)\n        optimizer.step()\n\n    avg_loss = np.average(losses)\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:22:47.380873Z","iopub.execute_input":"2024-07-02T21:22:47.381571Z","iopub.status.idle":"2024-07-02T21:22:47.390623Z","shell.execute_reply.started":"2024-07-02T21:22:47.381530Z","shell.execute_reply":"2024-07-02T21:22:47.389711Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def evaluate_text_recognizer(model,data_loader,device,blank_index) :\n    losses = []\n    model.eval()\n\n    for i in tqdm(data_loader) :\n        image = i[\"images\"].to(device).to(torch.float32)\n        targets = i[\"targets\"].to(device)\n        criteration = nn.CTCLoss(blank=blank_index,zero_infinity =True)\n\n        output = model(image)\n        output = output.permute(1,0,2)\n        output = output.log_softmax(2)\n        input_lengths = torch.tensor([output.size(0)] * output.size(1))\n        target_lengths = torch.tensor([targets.size(1)] * targets.size(0))\n\n        loss = criteration(output,targets,input_lengths,target_lengths)\n        losses.append(loss.item())\n    \n    avg_loss = np.average(losses)\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:22:48.089078Z","iopub.execute_input":"2024-07-02T21:22:48.089428Z","iopub.status.idle":"2024-07-02T21:22:48.096806Z","shell.execute_reply.started":"2024-07-02T21:22:48.089401Z","shell.execute_reply":"2024-07-02T21:22:48.095877Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# **Training loop**","metadata":{}},{"cell_type":"code","source":"for i in range(epochs) :\n    training_loss = train_text_recognizer(model=text_recognizer_model , data_loader= training_data_loader , \n                                          optimizer=optimizer ,device=device , blank_index = len(index_to_letters))\n    evaluation_loss = evaluate_text_recognizer(model=text_recognizer_model , data_loader= validation_data_loader , device=device, blank_index = len(index_to_letters))\n\n    if evaluation_loss < best_score :\n        best_score = evaluation_loss\n        torch.save(text_recognizer_model.state_dict(),'text_recognizer_best_model.bin')\n\n    print(f\"Epoch {i} :\\nTraining loss = {training_loss}\\nEvaluation loss = {evaluation_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T04:08:57.734846Z","iopub.execute_input":"2024-07-03T04:08:57.735591Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 406/471392 [00:20<6:28:37, 20.20it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}